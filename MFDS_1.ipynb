{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a0e6ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13560\\1737960041.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;31m# Run Mini-Batch Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mtheta_mini_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_copy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# Experiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13560\\1737960041.py\u001b[0m in \u001b[0;36mmini_batch_gradient_descent\u001b[1;34m(X, y, learning_rate, batch_size, num_iterations)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Randomly select a batch of samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Required Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute gradient\n",
    "def compute_gradient(X, y, theta):\n",
    "    m = len(y)\n",
    "    gradient = np.dot(X.T, (np.dot(X, theta) - y)) / m\n",
    "    return gradient\n",
    "\n",
    "# Vanilla Gradient Descent\n",
    "def vanilla_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        gradient = compute_gradient(X, y, theta)\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "# Mini-Batch Gradient Descent\n",
    "def mini_batch_gradient_descent(X, y, learning_rate, batch_size, num_iterations):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Randomly select a batch of samples\n",
    "        indices = np.random.choice(m, batch_size, replace=False)\n",
    "        X_batch = X[indices]\n",
    "        y_batch = y[indices]\n",
    "        \n",
    "        gradient = compute_gradient(X_batch, y_batch, theta)\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Randomly select a sample\n",
    "        index = np.random.randint(m)\n",
    "        X_sample = X[index]\n",
    "        y_sample = y[index]\n",
    "        \n",
    "        gradient = compute_gradient(X_sample, y_sample, theta)\n",
    "        theta = theta - learning_rate * gradient\n",
    "        \n",
    "    return theta\n",
    "\n",
    "# Example data generation\n",
    "num_samples = 100\n",
    "num_features = 15\n",
    "X = np.random.rand(num_samples, num_features)\n",
    "y = np.random.rand(num_samples)\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "batch_sizes = [1, 16, 32, 64, 128]  # Experiment with different batch sizes\n",
    "\n",
    "# Experiment\n",
    "num_samples, num_features = X.shape\n",
    "\n",
    "# Run experiments for different batch sizes\n",
    "for batch_size in batch_sizes:\n",
    "    # Clone the original data for each experiment to ensure fair comparison\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    \n",
    "    # Run Mini-Batch Gradient Descent\n",
    "    theta_mini_batch = mini_batch_gradient_descent(X_copy, y_copy, learning_rate, batch_size, num_iterations)\n",
    "    \n",
    "    # Experiment\n",
    "num_samples, num_features = X.shape\n",
    "\n",
    "# Run experiments for different batch sizes\n",
    "for batch_size in batch_sizes:\n",
    "    # Clone the original data for each experiment to ensure fair comparison\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    \n",
    "    # Run Mini-Batch Gradient Descent\n",
    "    theta_mini_batch = mini_batch_gradient_descent(X_copy, y_copy, learning_rate, batch_size, num_iterations)\n",
    "    \n",
    "    # Calculate mean squared error for the learned theta\n",
    "    mse = np.mean((X @ theta_mini_batch - y) ** 2)\n",
    "    \n",
    "    # Print convergence results for this batch size\n",
    "    print(f\"Batch Size: {batch_size}, MSE: {mse}\") \n",
    "    # Calculate convergence metrics (e.g., mean squared error) and plot convergence curves\n",
    "    # This will depend on your specific problem and dataset\n",
    "    # Print or plot convergence results for this batch size\n",
    "\n",
    "# Compare convergence results for different batch sizes\n",
    "# Choose the batch size that provides the best trade-off between convergence speed and stability\n",
    "# Plot batch size vs. convergence metric\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49828a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
